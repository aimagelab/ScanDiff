<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction">
  <meta name="keywords" content="Human Visual Attention, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./assets/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- Nuova riga con immagine + scritta sulla stessa linea -->
            <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
                <img src="assets/iccv2025_logo.svg" alt="ICCV Logo" style="height: 60px; margin-right: 0px;">
                <!-- <span style="font-size: 1.5rem; font-weight: 600; color: #5c5c5c;">ICCV 2025</span> -->
            </div>

            <div style="display: flex; align-items: center;">
              <img src="assets/scandiff_logo.png" alt="Icon" style="height: 100px; margin-right: 15px;">
              <h1 class="title is-1 publication-title">
                Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction
              </h1>
              <img src="assets/scandiff_logo.png" alt="Icon" style="height: 100px; margin-left: 15px;">
            </div>
            <!-- <h1 class="title is-1 publication-title">Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction</h1> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://giuseppecartella.github.io/">Giuseppe Cartella</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.vcuculo.com">Vittorio Cuculo</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/view/alessandro-damelio/home">Alessandro D'Amelio</a><sup>2</sup>,</span><br>
              <span class="author-block">
                <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella
                  Cornia</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LqM0uJwAAAAJ&hl">Giuseppe Boccignone</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.ritacucchiara.it/">Rita Cucchiara</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Modena and Reggio Emilia, Italy,</span>
              <span class="author-block"><sup>2</sup>University of Milan, Italy,</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.23021" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->

                <span class="link-block">
                  <a href="https://github.com/aimagelab/ScanDiff" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- YouTube Link -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=Kev1JMUA61Y" target="_blank" class="external-link button is-normal is-rounded is-danger">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video Presentation</span>
                  </a>
                </span>

                <!-- Dataset Link. -->

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <h2 class="subtitle has-text-centered">
          We propose <span class="scandiff">ScanDiff</span>, a unified architecture that integrates diffusion models with
          Vision Transformers to generate diverse and realistic gaze scanpaths. Unlike existing approaches, ScanDiff
          explicitly models scanpath variability by leveraging the stochastic nature of diffusion models, enabling
          the generation of diverse yet plausible gaze trajectories.
        </h2>
        <img src="assets/teaser.png" alt="ScanDiff Teaser" width="100%">
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Predicting human gaze scanpaths is crucial for understanding visual attention, 
              with applications in human-computer interaction, autonomous systems, and cognitive robotics. 
            </p>

            <p>
              While deep learning models have advanced scanpath prediction, most existing approaches generate averaged behaviors, 
              failing to capture the variability of human visual exploration. In this work, we present <span class="scandiff">ScanDiff</span>, a novel architecture
              that combines diffusion models with Vision Transformers to generate diverse and realistic scanpaths.
            </p>
            
            <p>
              Our method explicitly 
              models scanpath variability by leveraging the stochastic nature of diffusion models, producing a wide range of plausible gaze trajectories. 
              Additionally, we introduce textual conditioning to enable task-driven scanpath generation, allowing the model to adapt to different visual 
              search objectives.
            </p>

            <p>
              Experiments on benchmark datasets show that <span class="scandiff">ScanDiff</span> surpasses state-of-the-art methods in both free-viewing and task-driven 
              scenarios, producing more diverse and accurate scanpaths. These results highlight its ability to better capture the complexity of human visual behavior, 
              pushing forward gaze prediction research.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Method Overview -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Method Overview</h2>
          <div class="content has-text-centered">
            <img src="assets/model.png" alt="ScanDiff Method Overview" width="100%"><br><br>
            <p class="has-text-justified">
              <span class="scandiff">ScanDiff</span> is based on a unified architecture combining Diffusion Models with Transformers. 
              It represents the first diffusion-based approach for scanpath prediction on natural images. Textual conditioning allows <span class="scandiff">ScanDiff</span> to work both in free-viewing and task-driven scenarios,
              and a dedicated length prediction module is introduced to handle variable-length scanpaths.

              <h3>How does the model work?</h3>
              
            <!-- begine itemize with numbers-->
            <ol class="custom-steps has-text-left mt-4">
              <li><strong>Scanpath embedding:</strong> The scanpath is embedded into the initial uncorrupted latent variable 
                <span class="math-symbol">z<sub>0</sub></span>.
              </li>
              <li><strong>Forward diffusion:</strong> Gaussian noise is added to the embedded sequence 
                <span class="math-symbol">z<sub>0</sub></span> over <span class="math-symbol">T</span> timesteps.
              </li>
              <li><strong>Visual encoding:</strong> The stimulus <span class="math-symbol">I</span> is encoded with a Transformer-based backbone (DINOv2).
              </li>
              <li><strong>Task encoding:</strong> A textual encoder (CLIP) processes the viewing task <span class="math-symbol">c</span>.
              </li>
              <li><strong>Multimodal fusion:</strong> Visual and textual features are projected into a joint multimodal embedding space.
              </li>
              <li><strong>Denoising:</strong> A Transformer encoder refines the noised scanpath embedding 
                <span class="math-symbol">z<sub>t</sub></span>, conditioned on the multimodal features.
              </li>
              <li><strong>Reconstruction:</strong> A three-layer MLP 
                <span class="math-symbol">γ<sub>θ</sub></span> reconstructs the scanpath, and a length prediction module 
                <span class="math-symbol">ℓ<sub>θ</sub></span> estimates its length.
              </li>
            </ol>

            </p>
          </div>
        </div>
      </div>

      

      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
          <div class="content has-text-centered">
            <div class="carousel-container" style="width: 100%; margin: auto;">
              <div id="difference-detection-carousel" class="carousel results-carousel">
                <div class="item">
                  <img src="./assets/qualitatives/example1.png"
                    alt="Scandiff Qualitatives Example 1">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example1.png"
                    alt="Scandiff Qualitatives Example 1">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example2.png"
                    alt="Scandiff Qualitatives Example 2">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example3.png"
                    alt="Scandiff Qualitatives Example 3">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example4.png"
                    alt="Scandiff Qualitatives Example 4">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example5.png"
                    alt="Scandiff Qualitatives Example 5">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example6.png"
                    alt="Scandiff Qualitatives Example 6">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example7.png"
                    alt="Scandiff Qualitatives Example 7">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example8.png"
                    alt="Scandiff Qualitatives Example 8">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example9.png"
                    alt="Scandiff Qualitatives Example 9">
                </div>
                <div class="item">
                  <img src="./assets/qualitatives/example10.png"
                    alt="Scandiff Qualitatives Example 10">
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Scanpath Variability Analysis</h2>
          <div class="content has-text-justified">
            <p>
              Human visual exploration is inherently variable. Individuals perceive the same stimulus in different manners depending on factors such as attention, 
              context, and cognitive processes. Capturing such variability is essential for developing models that accurately reflect the diverse range of
              human traits. However, existing scanpath prediction models tend to align closely with the statistical mean of human gaze behavior. While this approach may improve performance
              on traditional evaluation metrics, it fails to reflect the natural variability in human visual attention. Commonly used
              metrics such as MM, SM, and SS tend to reward predictions that closely match an aggregated ground truth, thus
              favoring models that generate a single representative scanpath. Indeed, the
              average similarity between ground-truth scanpaths can be smaller than the average similarity between generated scanpaths if these well reflect an average behavior.
              <br><br>
              We propose the <strong>Diversity-aware Sequence Score (DSS)</strong>, a new metric that extends the standard sequence similarity mesures by incorporating
              a term that penalizes excessive similarity among the generated scanpaths when humans do not reflect such behavior. Given a set of generated scanpaths <span class="math-symbol">s<sub>g</sub></span> and 
              corresponding human scanpaths <span class="math-symbol">s<sub>h</sub></span> for a specific visual stimulus, DSS is computed as:
            </p>

            <div class="has-text-centered my-5">
              <figure class="image is-inline-block">
                <img src="assets/metric.png" alt="metric" style="width: 40%;">
              </figure>
            </div>

            
            <div class="carousel-container" style="width: 100%; margin: auto;">
              <div id="difference-detection-carousel" class="carousel results-carousel">
                <div class="item">
                  <img src="./assets/variability_imgs/example1.png"
                    alt="Scandiff Variability Example 1">
                </div>
                <div class="item">
                  <img src="./assets/variability_imgs/example2.png"
                    alt="Scandiff Variability Example 2">
                </div>
                <div class="item">
                  <img src="./assets/variability_imgs/example3.png"
                    alt="Scandiff Variability Example 3">
                </div>
                <div class="item">
                  <img src="./assets/variability_imgs/example4.png"
                    alt="Scandiff Variability Example 4">
                </div>
              </div>
            </div>
            <br>
            This is a first attempt to quantitatively assess the ability of a model to generate diverse, yet human-like, gaze trajectories.
            <span class="scandiff">ScanDiff</span> achieves the best overall performance on all settings and datasets, highlighting its
            effectiveness in predicting accurate eye movement trajectories well aligned with the human scanpath variability.
            Goal-oriented scanpaths tend to be more deterministic, particularly in the target-present setting, and are generally shorter
            than those in free-viewing scenarios. Nevertheless, our model effectively captures even the more subtle variability present in human gaze behavior.
          </div>
        </div>
      </div>
      <!--/ Method Overview -->

    </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{cartella2025modeling,
  title     = {Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction},
  author    = {Cartella, Giuseppe and Cuculo, Vittorio and D'Amelio, Alessandro and Cornia, Marcella and Boccignone, Giuseppe and Cucchiara, Rita},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!--         <a class="icon-link" href="#">
          <i class="fas fa-file-pdf"></i>
        </a> -->
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
